# ğŸ“¡ è®ºæ–‡çˆ¬è™«ä½¿ç”¨æŒ‡å—

## âœ… åŠŸèƒ½æ¦‚è¿°

ç»¼åˆè®ºæ–‡çˆ¬è™«æœåŠ¡ï¼Œä»å¤šä¸ªæ¸ é“è‡ªåŠ¨è·å–æœ€æ–°çƒ­é—¨AIè®ºæ–‡ï¼š

### æ”¯æŒçš„æ•°æ®æº

1. **arXiv API** âœ… ï¼ˆç¨³å®šï¼Œæ¨èï¼‰
   - ç›´æ¥é€šè¿‡å®˜æ–¹APIè·å–
   - æ— éœ€è®¤è¯ï¼Œæ— é¢‘ç‡é™åˆ¶
   - å®æ—¶æ›´æ–°ï¼Œæ•°æ®å®Œæ•´

2. **Reddit** âš ï¸ ï¼ˆéœ€è¦é…ç½®ï¼‰
   - r/MachineLearning
   - r/artificial  
   - r/deeplearning
   - **æ³¨æ„**: Redditæœ‰åçˆ¬è™«æœºåˆ¶ï¼Œå»ºè®®ä½¿ç”¨å®˜æ–¹API

3. **Papers with Code** âš ï¸ ï¼ˆéœ€è¦é…ç½®ï¼‰
   - å¸¦ä»£ç çš„çƒ­é—¨è®ºæ–‡
   - **æ³¨æ„**: å¯èƒ½éœ€è¦ä»£ç†æˆ–APIå¯†é’¥

4. **Hugging Face Papers** âš ï¸ ï¼ˆéœ€è¦é…ç½®ï¼‰
   - æ¯æ—¥ç²¾é€‰è®ºæ–‡
   - **æ³¨æ„**: å¯èƒ½éœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥

5. **Twitter/X** âš ï¸ ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
   - AIè®ºæ–‡ç›¸å…³æ¨æ–‡
   - **éœ€è¦**: `TWITTER_BEARER_TOKEN`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šä½¿ç”¨arXiv APIï¼ˆæ¨èï¼‰

arXivæ˜¯æœ€ç¨³å®šçš„æ•°æ®æºï¼Œæ— éœ€é¢å¤–é…ç½®ï¼š

```bash
# è®¿é—®å‰ç«¯
http://localhost:3000/papers

# ç‚¹å‡»"åˆ·æ–°"æŒ‰é’®ï¼ˆğŸ”„å›¾æ ‡ï¼‰
# è‡ªåŠ¨ä»arXivè·å–æœ€æ–°è®ºæ–‡
```

### æ–¹æ³•2ï¼šç»¼åˆçˆ¬å–ï¼ˆéœ€è¦é…ç½®ï¼‰

å¦‚æœè¦ä½¿ç”¨Redditã€Papers with Codeç­‰æ¸ é“ï¼š

1. **é…ç½®Reddit API**ï¼ˆæ¨èï¼‰

```bash
# 1. è®¿é—® https://www.reddit.com/prefs/apps
# 2. åˆ›å»ºåº”ç”¨è·å– client_id å’Œ client_secret
# 3. åœ¨ .env ä¸­æ·»åŠ ï¼š
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=your_app_name/1.0
```

2. **é…ç½®Twitter API**ï¼ˆå¯é€‰ï¼‰

```bash
# 1. è®¿é—® https://developer.twitter.com/
# 2. åˆ›å»ºåº”ç”¨è·å– Bearer Token
# 3. åœ¨ .env ä¸­æ·»åŠ ï¼š
TWITTER_BEARER_TOKEN=your_bearer_token
```

---

## ğŸ”§ å½“å‰é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: Redditè¿”å›403 Forbidden

**åŸå› **: Redditæ£€æµ‹åˆ°çˆ¬è™«è¡Œä¸º

**è§£å†³æ–¹æ¡ˆ**:

1. **ä½¿ç”¨Redditå®˜æ–¹API**ï¼ˆæ¨èï¼‰

ä¿®æ”¹ `paperCrawlerService.js` ä¸­çš„Redditçˆ¬å–éƒ¨åˆ†ï¼š

```javascript
// ä½¿ç”¨OAuthè®¤è¯
const response = await axios.get(
  `https://oauth.reddit.com/r/${subreddit}/hot`,
  {
    headers: {
      'Authorization': `Bearer ${accessToken}`,
      'User-Agent': process.env.REDDIT_USER_AGENT
    }
  }
);
```

2. **æ·»åŠ æ›´çœŸå®çš„è¯·æ±‚å¤´**

```javascript
headers: {
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en-US,en;q=0.9',
  'Accept-Encoding': 'gzip, deflate, br',
  'Connection': 'keep-alive',
  'Upgrade-Insecure-Requests': '1'
}
```

3. **ä½¿ç”¨ä»£ç†**ï¼ˆå¦‚æœåœ¨å›½å†…ï¼‰

```bash
# è®¾ç½®HTTPä»£ç†
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port
```

### é—®é¢˜2: Papers with Code / Hugging Face è¿æ¥è¢«é‡ç½®

**åŸå› **: ç½‘ç»œä¸ç¨³å®šæˆ–è¢«é™æµ

**è§£å†³æ–¹æ¡ˆ**:

1. **æ·»åŠ é‡è¯•æœºåˆ¶**

```javascript
async function fetchWithRetry(url, options, retries = 3) {
  for (let i = 0; i < retries; i++) {
    try {
      return await axios.get(url, options);
    } catch (error) {
      if (i === retries - 1) throw error;
      await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)));
    }
  }
}
```

2. **å¢åŠ è¶…æ—¶æ—¶é—´**

```javascript
timeout: 30000  // 30ç§’
```

3. **ä½¿ç”¨å¤‡ç”¨æº**

å¦‚æœå¤–éƒ¨APIä¸ç¨³å®šï¼Œä¸“æ³¨ä½¿ç”¨arXivï¼š

```javascript
// è·å–æ›´å¤šarXivè®ºæ–‡åˆ†ç±»
const categories = [
  'cs.AI',   // äººå·¥æ™ºèƒ½
  'cs.LG',   // æœºå™¨å­¦ä¹ 
  'cs.CV',   // è®¡ç®—æœºè§†è§‰
  'cs.CL',   // è‡ªç„¶è¯­è¨€å¤„ç†
  'cs.NE',   // ç¥ç»ç½‘ç»œ
  'cs.RO',   // æœºå™¨äºº
  'stat.ML'  // ç»Ÿè®¡æœºå™¨å­¦ä¹ 
];
```

---

## ğŸ“Š æ¨èé…ç½®

### é…ç½®1: ç¨³å®šç‰ˆï¼ˆåªç”¨arXivï¼‰

é€‚åˆå¿«é€Ÿéƒ¨ç½²ï¼Œæ— éœ€é¢å¤–é…ç½®ï¼š

```javascript
// server/routes/papers.js
router.post('/papers/crawl', async (req, res) => {
  // ä»arXivè·å–æ›´å¤šåˆ†ç±»çš„è®ºæ–‡
  const categories = ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'cs.NE', 'cs.RO'];
  const result = await arxivService.fetchMultiCategoryPapers(categories, 30, true);
  // ...
});
```

### é…ç½®2: å®Œæ•´ç‰ˆï¼ˆåŒ…å«ç¤¾äº¤åª’ä½“ï¼‰

éœ€è¦é…ç½®APIå¯†é’¥ï¼š

```javascript
// å¯ç”¨æ‰€æœ‰æ¸ é“
{
  reddit: true,          // éœ€è¦Reddit API
  papersWithCode: true,  // éœ€è¦ç¨³å®šç½‘ç»œ
  huggingface: true,     // éœ€è¦ç¨³å®šç½‘ç»œ
  twitter: true,         // éœ€è¦Twitter API
  limit: 20
}
```

---

## ğŸ¯ å®ç”¨å»ºè®®

### 1. ä»arXivå¼€å§‹

arXivæ˜¯æœ€å¯é çš„è®ºæ–‡æ¥æºï¼š

```bash
# ç‚¹å‡»"åˆ·æ–°"æŒ‰é’®
# è‡ªåŠ¨è·å– cs.AI, cs.LG, cs.CV, cs.CL å››ä¸ªåˆ†ç±»çš„æœ€æ–°è®ºæ–‡
# æ¯ä¸ªåˆ†ç±»15ç¯‡ï¼Œå…±60ç¯‡
```

### 2. æ‰‹åŠ¨çˆ¬å–æµ‹è¯•

```bash
cd server
node test-crawler.js
```

### 3. æŸ¥çœ‹çˆ¬å–çŠ¶æ€

```bash
curl http://localhost:5000/api/papers/crawl-status
```

### 4. å®šæ—¶çˆ¬å–ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦è‡ªåŠ¨æ›´æ–°ï¼Œå¯ä»¥ä½¿ç”¨ `node-cron`ï¼š

```javascript
const cron = require('node-cron');

// æ¯6å°æ—¶çˆ¬å–ä¸€æ¬¡
cron.schedule('0 */6 * * *', async () => {
  console.log('ğŸ• å®šæ—¶çˆ¬å–è®ºæ–‡...');
  await paperCrawlerService.crawlAllSources({
    reddit: false,
    papersWithCode: false,
    huggingface: false,
    twitter: false,
    limit: 20
  });
});
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ä¼˜å…ˆä½¿ç”¨å®˜æ–¹API

- âœ… arXiv API
- âœ… Reddit OAuth API
- âœ… Twitter API v2
- âŒ ç›´æ¥çˆ¬å–HTMLï¼ˆå®¹æ˜“è¢«å°ï¼‰

### 2. å°Šé‡robots.txt

æ£€æŸ¥ç½‘ç«™çš„çˆ¬è™«æ”¿ç­–ï¼š

```bash
curl https://www.reddit.com/robots.txt
curl https://paperswithcode.com/robots.txt
```

### 3. æ·»åŠ å»¶è¿Ÿå’Œé™æµ

é¿å…é¢‘ç¹è¯·æ±‚ï¼š

```javascript
await new Promise(resolve => setTimeout(resolve, 1000)); // 1ç§’å»¶è¿Ÿ
```

### 4. ç¼“å­˜ç»“æœ

å‡å°‘ä¸å¿…è¦çš„è¯·æ±‚ï¼š

```javascript
const cache = new NodeCache({ stdTTL: 3600 }); // 1å°æ—¶ç¼“å­˜
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. æŸ¥çœ‹æ—¥å¿—

```bash
tail -f /tmp/server.log
```

### 2. æµ‹è¯•å•ä¸ªæ¸ é“

```javascript
// åªæµ‹è¯•Reddit
const result = await paperCrawlerService.fetchFromReddit(10);
console.log(result);
```

### 3. æ£€æŸ¥ç½‘ç»œè¿æ¥

```bash
curl -I https://www.reddit.com/r/MachineLearning/hot.json
curl -I https://paperswithcode.com/api/v1/papers/
curl -I https://huggingface.co/api/daily_papers
```

---

## ğŸ“ æ•°æ®åº“

çˆ¬å–çš„è®ºæ–‡ä¼šè‡ªåŠ¨ä¿å­˜åˆ°MongoDBï¼š

```javascript
// æŸ¥çœ‹æ•°æ®åº“ä¸­çš„è®ºæ–‡
use ai_teacher_db
db.papers.find().limit(10)

// ç»Ÿè®¡
db.papers.countDocuments()
db.papers.countDocuments({ trending: true })
```

---

## ğŸš¦ å½“å‰çŠ¶æ€

- âœ… **åç«¯æœåŠ¡**: å·²å¯åŠ¨
- âœ… **å‰ç«¯ç•Œé¢**: "çˆ¬å–çƒ­é—¨"æŒ‰é’®å·²æ·»åŠ 
- âœ… **arXivçˆ¬å–**: æ­£å¸¸å·¥ä½œ
- âš ï¸ **Redditçˆ¬å–**: éœ€è¦é…ç½®API
- âš ï¸ **Papers with Code**: éœ€è¦ç¨³å®šç½‘ç»œ
- âš ï¸ **Hugging Face**: éœ€è¦ç¨³å®šç½‘ç»œ
- âŒ **Twitterçˆ¬å–**: éœ€è¦APIå¯†é’¥

---

## ğŸ‰ ç«‹å³ä½“éªŒ

### æ–¹æ³•1: ä½¿ç”¨ç°æœ‰åŠŸèƒ½

1. è®¿é—® http://localhost:3000/papers
2. ç‚¹å‡»"åˆ·æ–°"æŒ‰é’®ï¼ˆğŸ”„ï¼‰
3. è‡ªåŠ¨ä»arXivè·å–æœ€æ–°è®ºæ–‡

### æ–¹æ³•2: é…ç½®å®Œæ•´çˆ¬è™«

1. æŒ‰ç…§ä¸Šè¿°æŒ‡å—é…ç½®APIå¯†é’¥
2. ç‚¹å‡»"çˆ¬å–çƒ­é—¨"æŒ‰é’®
3. ä»å¤šä¸ªæ¸ é“è·å–çƒ­é—¨è®ºæ–‡

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æ£€æŸ¥ `/tmp/server.log` æ—¥å¿—
2. è¿è¡Œ `node test-crawler.js` æµ‹è¯•
3. æŸ¥çœ‹ API å“åº”çŠ¶æ€ç 
4. ç¡®è®¤ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®

---

**æ¨è**: ç›®å‰æœ€ç¨³å®šçš„æ–¹å¼æ˜¯ä½¿ç”¨ arXiv APIï¼Œæ— éœ€é¢å¤–é…ç½®ï¼Œæ•°æ®è´¨é‡é«˜ä¸”å®æ—¶æ›´æ–°ã€‚

